{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f39a8fb",
   "metadata": {},
   "source": [
    "# KV Cache Memory Model\n",
    "\n",
    "This notebook models the memory usage of the KV cache for a decoder only transformer\n",
    "\n",
    "We explore how memory scales with\n",
    "\n",
    "1) Number of Layers\n",
    "2) Batch Size\n",
    "3) Sequence Length (context length)\n",
    "4) Hidden Dimension\n",
    "5) Precision (FP16 vs FP8)\n",
    "\n",
    "The goal is to analyze how KV cache dominates memory as context length and batch size grow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e780e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b034ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kv_cache_memory_bytes(\n",
    "    num_layers: int,\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "    hidden_dim: int, \n",
    "    bytes_per_element: int=2, #FP16 default\n",
    ") -> int:\n",
    "\n",
    "    \"\"\" \n",
    "        Approximate KV cache memory in bytes\n",
    "        Formula: \n",
    "        memory = 2(K and V) * L *B*T*D* bytes_per_element\n",
    "\n",
    "        Where \n",
    "\n",
    "        L = num_layers      (number of transformer layers)\n",
    "        B = batch_size      (how many sequences processed at once)\n",
    "        T = seq_len         (context window / sequence length)\n",
    "        D = hidden_dim      (model width)\n",
    "        bytes_per_element   (1=FP8, 2=FP16/BF16, 4=FP32)\n",
    "\n",
    "\"\"\"\n",
    "    return 2* num_layers * batch_size *seq_len * hidden_dim * bytes_per_element\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d0e4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294967296"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example model parameters (7B-class model)\n",
    "num_layers = 32       # L\n",
    "batch_size = 4        # B\n",
    "seq_len = 2048        # T \n",
    "hidden_dim = 4096     # D \n",
    "precision_bytes = 2   # FP16\n",
    "\n",
    "# Calculate raw KV cache memory in bytes\n",
    "mem_bytes = kv_cache_memory_bytes(\n",
    "    num_layers,\n",
    "    batch_size,\n",
    "    seq_len,\n",
    "    hidden_dim,\n",
    "    precision_bytes,\n",
    ")\n",
    "\n",
    "mem_bytes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
